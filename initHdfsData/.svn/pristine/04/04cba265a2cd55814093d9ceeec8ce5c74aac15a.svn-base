package com.ttmv.datacenter;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.*;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.List;

/**
 * hadoop操作hdfs工具类
 * 
 * @author zkt
 *
 */
public class HadoopFSOperations {
	private static Logger logger = LogManager.getLogger(HadoopFSOperations.class);

	private HadoopFSOperations() {

	}

	static Configuration hdfsConfig = new Configuration();
	// static Configuration hbseConfig = HBaseConfiguration.create();

	/*
	 * upload the local file to the hds notice that the path is full like
	 * /tmp/test.c
	 */
	public static void uploadLocalFile2HDFS(String s, String d) throws IOException {
		FileSystem hdfs = FileSystem.get(hdfsConfig);
		Path src = new Path(s);
		Path dst = new Path(d);
		hdfs.copyFromLocalFile(src, dst);
		hdfs.close();
	}

	public static boolean isDirExit(String tarPath) throws Exception {
		FileSystem hdfs = FileSystem.newInstance(hdfsConfig);
		Path path = new Path(tarPath);
		if (hdfs.exists(path)) {
			return true;
		}
		hdfs.close();
		return false;
	}

	/*
	 * create a new file in the hdfs. notice that the toCreateFilePath is the
	 * full path and write the content to the hdfs file.
	 */
	public static  void  createNewHDFSFile(String toCreateFilePath, String content) throws IOException {
		FileSystem hdfs = FileSystem.newInstance(hdfsConfig);
		Path path = new Path(toCreateFilePath);
		// 如果文件不存在，则先创建文件
		if (!hdfs.exists(path)) {
			hdfs.create(path).close();
		}
		// if path not exit,it will be create it
		FSDataOutputStream os = hdfs.append(path);
		os.write(content.getBytes("UTF-8"));
		// start a newline
		os.write("\n".getBytes());
		// os.flush();
		os.close();
		//hdfs.close();
	}

	/*
	 * delete the hdfs file notice that the dst is the full path name
	 */
	public static boolean deleteHDFSFile(String dst) throws IOException {
		FileSystem hdfs = FileSystem.get(hdfsConfig);
		Path path = new Path(dst);
		boolean isDeleted = hdfs.deleteOnExit(path);
		hdfs.close();
		return isDeleted;
	}

	/*
	 * read the hdfs file content notice that the dst is the full path name
	 */
	public static byte[] readHDFSFile(String dst) throws Exception {
		FileSystem fs = FileSystem.newInstance(hdfsConfig);
		// check if the file exists
		Path path = new Path(dst);
		if (fs.exists(path)) {
			FSDataInputStream is = fs.open(path);
			// get the file info to create the buffer
			FileStatus stat = fs.getFileStatus(path);
			// create the buffer
			byte[] buffer = new byte[Integer.parseInt(String.valueOf(stat.getLen()))];
			is.readFully(0, buffer);
			is.close();
			fs.close();

			return buffer;
		} else {
			throw new Exception("the file is not found .");
		}
	}

	/*
	 * make a new dir in the hdfs
	 * 
	 * the dir may like '/tmp/testdir'
	 */
	public static void mkdir(String dir) throws IOException {
		FileSystem fs = FileSystem.get(hdfsConfig);
		fs.mkdirs(new Path(dir));
		fs.close();
	}

	/*
	 * delete a dir in the hdfs
	 * 
	 * dir may like '/tmp/testdir'
	 */
	public static void deleteDir(String dir) throws IOException {
		FileSystem fs = FileSystem.get(hdfsConfig);
		fs.deleteOnExit(new Path(dir));
		fs.close();
	}

	public static List<String> readHdfsDate(String dir) throws Exception {
		List<String> dataList = new ArrayList<String>();
		// 流读入和写入
		InputStream in = null;
		// 使用缓冲流，进行按行读取的功能
		BufferedReader buff = null;
		// 获取HDFS的conf
		FileSystem fs = FileSystem.newInstance(hdfsConfig);
		Path path = new Path(dir);
		if (fs.exists(path)) {
			FileStatus[] stats = fs.listStatus(path);
			for (int i = 0; i < stats.length; ++i) {
				if (stats[i].isFile()) {
					Path p = new Path(stats[i].getPath().toString());
					// 打开文件流
					in = fs.open(p);
					// BufferedReader包装一个流
					buff = new BufferedReader(new InputStreamReader(in));
					String str = null;
					while ((str = buff.readLine()) != null) {
						logger.debug("从hdfs" + dir + "路径下读取文件:" + stats[i].getPath().toString() + "内容：" + str);
						dataList.add(str);
					}
					buff.close();
				}
			}
			in.close();
			fs.close();
		}
		return dataList;
	}



	private static SimpleDateFormat sdf = new SimpleDateFormat("yyyyMMdd");



}
